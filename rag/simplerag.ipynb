{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Verify that the API key is set\n",
    "print(os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PDF Reader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(\"Kaavish_Proposal.pdf\")\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text Splitter to break down PDF into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x24d3e9b7da0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, Column, Integer, Text, Numeric\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Database URL\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\", \"postgresql://neondb_owner:npg_wNTU4WiDqJQ5@ep-square-tooth-a1szbuny-pooler.ap-southeast-1.aws.neon.tech/neondb?sslmode=require\")\n",
    "\n",
    "# Create database engine\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Create a configured \"Session\" class\n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "# Create a Session\n",
    "session = Session()\n",
    "\n",
    "# Define the base class for declarative class definitions\n",
    "Base = declarative_base()\n",
    "\n",
    "# Define the table schema\n",
    "class Product(Base):\n",
    "    __tablename__ = 'shahzar_table'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    name = Column(Text, nullable=False)\n",
    "    price = Column(Numeric(10, 2), nullable=False)\n",
    "    image = Column(Text, nullable=False)\n",
    "    url = Column(Text, nullable=False)\n",
    "    store = Column(Text, nullable=False)\n",
    "\n",
    "# Create the table (if it doesn't exist)\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "# Example function to query the database\n",
    "def get_all_products():\n",
    "    results = session.query(Product).all()\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "products = get_all_products()\n",
    "for product in products:\n",
    "    print({\n",
    "        \"id\": product.id,\n",
    "        \"name\": product.name,\n",
    "        \"price\": str(product.price),\n",
    "        \"image\": product.image,\n",
    "        \"url\": product.url,\n",
    "        \"store\": product.store\n",
    "    })\n",
    "\n",
    "# Create initial Document objects\n",
    "initial_documents = [\n",
    "    Document(\n",
    "        metadata={\n",
    "            \"id\": product.id,\n",
    "            \"name\": product.name,\n",
    "            \"price\": str(product.price),\n",
    "            \"image\": product.image,\n",
    "            \"url\": product.url,\n",
    "            \"store\": product.store\n",
    "        },\n",
    "        page_content=f\"{product.name} {product.price} {product.store}\"\n",
    "    )\n",
    "    for product in products\n",
    "]\n",
    "\n",
    "# Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(initial_documents)\n",
    "\n",
    "# Create embeddings and vector database\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_db = FAISS.from_documents(documents, embeddings)\n",
    "vector_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black Iconic POLO 3290.00 Fitted Shop\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"Black\"\n",
    "results = vector_db.similarity_search(query)\n",
    "print(results[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000024D3E8221E0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000024D3E823FB0>, root_client=<openai.OpenAI object at 0x0000024D2FE8A3C0>, root_async_client=<openai.AsyncOpenAI object at 0x0000024D3E822240>, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following questions based on only the provided context. \n",
    "Think step by step before providing a detailed answer. \n",
    "I will tip you $1000 if the user finds your answer helpful.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Document Chain \n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000024D3E9B7DA0>, search_kwargs={})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retrievers: A retriever is an interface that returns documents given\n",
    " an unstructured query. It is more general than a vector store.\n",
    " A retriever does not need to be able to store documents, only to \n",
    " return (or retrieve) them. Vector stores can be used as the backbone\n",
    " of a retriever, but there are other types of retrievers as well. \n",
    " https://python.langchain.com/docs/modules/data_connection/retrievers/   \n",
    "\"\"\"\n",
    "\n",
    "retriever=vector_db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieval chain:This chain takes in a user inquiry, which is then\n",
    "passed to the retriever to fetch relevant documents. Those documents \n",
    "(and original inputs) are then passed to an LLM to generate a response\n",
    "https://python.langchain.com/docs/modules/chains/\n",
    "\"\"\"\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=retrieval_chain.invoke({\"input\":\"Tell me about the most expensive black shirt?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the most expensive black shirt is the Black Iconic POLO priced at 3290.00.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
